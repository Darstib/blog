{
  "version": "0.7.28",
  "endpoint": "https://api.moonshot.cn",
  "models": [],
  "api_key": "",
  "encrypt_keys": false,
  "selectedProvider": "Google GenerativeAI (Langchain)",
  "max_tokens": 500,
  "temperature": 0.7,
  "frequency_penalty": 0.5,
  "showStatusBar": true,
  "outputToBlockQuote": false,
  "freeCursorOnStreaming": false,
  "allowJavascriptRun": false,
  "experiment": false,
  "promptsPath": "textgenerator/templates",
  "textGenPath": "textgenerator/",
  "prefix": "\n\n",
  "tgSelectionLimiter": "^\\*\\*\\*",
  "stream": true,
  "context": {
    "customInstructEnabled": true,
    "includeClipboard": true,
    "customInstruct": "Title: {{title}}\n  \nStarred Blocks: {{starredBlocks}}\n\t  \n{{tg_selection}}",
    "contextTemplate": "Title: {{title}}\n\t\nStarred Blocks: {{starredBlocks}}\n\t  \n{{tg_selection}}"
  },
  "requestTimeout": 300000,
  "options": {
    "generate-text": true,
    "generate-text-with-metadata": true,
    "insert-generated-text-From-template": true,
    "create-generated-text-From-template": false,
    "search-results-batch-generate-from-template": true,
    "insert-text-From-template": false,
    "create-text-From-template": false,
    "show-modal-From-template": true,
    "open-template-as-tool": true,
    "open-playground": true,
    "set_max_tokens": true,
    "set-llm": true,
    "set-model": true,
    "packageManager": true,
    "create-template": false,
    "get-title": true,
    "generated-text-to-clipboard-From-template": false,
    "calculate-tokens": true,
    "calculate-tokens-for-template": true,
    "text-extractor-tool": true,
    "stop-stream": true,
    "custom-instruct": true,
    "generate-in-right-click-menu": false,
    "batch-generate-in-right-click-files-menu": true,
    "tg-block-processor": true,
    "reload": true,
    "disable-ribbon-icons": false
  },
  "advancedOptions": {
    "generateTitleInstructEnabled": false,
    "generateTitleInstruct": "Generate a title for the current document (do not use * \" \\ / < > : | ? .):\n{{substring content 0 255}}",
    "includeAttachmentsInRequest": false
  },
  "autoSuggestOptions": {
    "customInstructEnabled": true,
    "customInstruct": "Continue the follwing text:\nTitle: {{title}}\n{{query}}",
    "isEnabled": false,
    "allowInNewLine": false,
    "delay": 300,
    "numberOfSuggestions": 5,
    "triggerPhrase": "  ",
    "stop": ".",
    "showStatus": true,
    "customProvider": false,
    "inlineSuggestions": false,
    "overrideTrigger": " "
  },
  "slashSuggestOptions": {
    "isEnabled": false,
    "triggerPhrase": "/"
  },
  "extractorsOptions": {
    "PDFExtractor": true,
    "WebPageExtractor": true,
    "YoutubeExtractor": true,
    "AudioExtractor": false,
    "ImageExtractorEmbded": true,
    "ImageExtractor": true
  },
  "displayErrorInEditor": true,
  "LLMProviderProfiles": {
    "Default (Custom) 1": {
      "extends": "Default (Custom)",
      "name": "Default 1"
    },
    "Default (Custom) 1 2": {
      "extends": "Default (Custom)",
      "name": "Default 2"
    },
    "Google GenerativeAI (Langchain) 1": {
      "extends": "Google GenerativeAI (Langchain)",
      "name": "Google GenerativeAI 1"
    }
  },
  "LLMProviderOptions": {
    "whisper": {
      "base_path": "https://api.openai.com/v1",
      "model": "whisper-1"
    },
    "OpenAI Chat (Langchain)": {
      "basePath": "https://api.openai-hk.com/v1",
      "model": "gpt-3.5-turbo",
      "api_key": ""
    },
    "Default (Custom)": {
      "endpoint": "https://api.openai.com/v1/chat/completions",
      "custom_header": "{\n    \"Content-Type\": \"application/json\",\n    authorization: \"Bearer {{api_key}}\"\n}",
      "custom_body": "{\n    model: \"{{model}}\",\n    temperature: {{temperature}},\n    top_p: {{top_p}},\n    frequency_penalty: {{frequency_penalty}},\n    presence_penalty: {{presence_penalty}},\n    max_tokens: {{max_tokens}},\n    n: {{n}},\n    stream: {{stream}},\n    stop: \"{{stop}}\",\n    messages: {{stringify messages}}\n}",
      "model": "gpt-3.5-turbo-16k",
      "sanatization_streaming": "// catch error\nif (res.status >= 300) {\n  const err = data?.error?.message || JSON.stringify(data);\n  throw err;\n}\nlet resultTexts = [];\nconst lines = this.chunk.split(\"\\ndata: \");\n\nconst parsedLines = lines\n    .map((line) => line.replace(/^data: /, \"\").trim()) // Remove the \"data: \" prefix\n    .filter((line) => line !== \"\" && line !== \"[DONE]\") // Remove empty lines and \"[DONE]\"\n    .map((line) => {\n        try {\n            return JSON.parse(line)\n        } catch { }\n    }) // Parse the JSON string\n    .filter(Boolean);\n\nfor (const parsedLine of parsedLines) {\n    const { choices } = parsedLine;\n    const { delta } = choices[0];\n    const { content } = delta;\n    // Update the UI with the new content\n    if (content) {\n        resultTexts.push(content);\n    }\n}\nreturn resultTexts.join(\"\");",
      "sanatization_response": "// catch error\nif (res.status >= 300) {\n  const err = data?.error?.message || JSON.stringify(data);\n  throw err;\n}\n\n// get choices\nconst choices = (data.choices || data).map(c=> c.message);\n\n// the return object should be in the format of \n// { content: string }[] \n// if there's only one response, put it in the array of choices.\nreturn choices;",
      "frequency_penalty": 0,
      "presence_penalty": 0.5,
      "top_p": 1,
      "api_key": ""
    },
    "Google GenerativeAI (Langchain)": {
      "model": "gemini-1.5-flash",
      "api_key": ""
    },
    "OpenAI Instruct (Langchain)": {
      "basePath": "https://api.moonshot.cn",
      "model": "gpt-3.5-turbo-instruct"
    },
    "Huggingface (Langchain)": {},
    "Default (Custom) 1": {
      "endpoint": "https://api.openai.com/v1/chat/completions",
      "custom_header": "{\n    \"Content-Type\": \"application/json\",\n    authorization: \"Bearer {{api_key}}\"\n}",
      "custom_body": "{\n    model: \"{{model}}\",\n    temperature: {{temperature}},\n    top_p: {{top_p}},\n    frequency_penalty: {{frequency_penalty}},\n    presence_penalty: {{presence_penalty}},\n    max_tokens: {{max_tokens}},\n    n: {{n}},\n    stream: {{stream}},\n    stop: \"{{stop}}\",\n    messages: {{stringify messages}}\n}",
      "model": "gpt-3.5-turbo-16k",
      "sanatization_streaming": "// catch error\nif (res.status >= 300) {\n  const err = data?.error?.message || JSON.stringify(data);\n  throw err;\n}\nlet resultTexts = [];\nconst lines = this.chunk.split(\"\\ndata: \");\n\nconst parsedLines = lines\n    .map((line) => line.replace(/^data: /, \"\").trim()) // Remove the \"data: \" prefix\n    .filter((line) => line !== \"\" && line !== \"[DONE]\") // Remove empty lines and \"[DONE]\"\n    .map((line) => {\n        try {\n            return JSON.parse(line)\n        } catch { }\n    }) // Parse the JSON string\n    .filter(Boolean);\n\nfor (const parsedLine of parsedLines) {\n    const { choices } = parsedLine;\n    const { delta } = choices[0];\n    const { content } = delta;\n    // Update the UI with the new content\n    if (content) {\n        resultTexts.push(content);\n    }\n}\nreturn resultTexts.join(\"\");",
      "sanatization_response": "// catch error\nif (res.status >= 300) {\n  const err = data?.error?.message || JSON.stringify(data);\n  throw err;\n}\n\n// get choices\nconst choices = (data.choices || data).map(c=> c.message);\n\n// the return object should be in the format of \n// { content: string }[] \n// if there's only one response, put it in the array of choices.\nreturn choices;",
      "frequency_penalty": 0,
      "presence_penalty": 0.5,
      "top_p": 1,
      "api_key": ""
    },
    "Default (Custom) 1 2": {
      "endpoint": "https://api.openai.com/v1/chat/completions",
      "custom_header": "{\n    \"Content-Type\": \"application/json\",\n    authorization: \"Bearer {{api_key}}\"\n}",
      "custom_body": "{\n    model: \"{{model}}\",\n    temperature: {{temperature}},\n    top_p: {{top_p}},\n    frequency_penalty: {{frequency_penalty}},\n    presence_penalty: {{presence_penalty}},\n    max_tokens: {{max_tokens}},\n    n: {{n}},\n    stream: {{stream}},\n    stop: \"{{stop}}\",\n    messages: {{stringify messages}}\n}",
      "model": "gpt-3.5-turbo-16k",
      "sanatization_streaming": "// catch error\nif (res.status >= 300) {\n  const err = data?.error?.message || JSON.stringify(data);\n  throw err;\n}\nlet resultTexts = [];\nconst lines = this.chunk.split(\"\\ndata: \");\n\nconst parsedLines = lines\n    .map((line) => line.replace(/^data: /, \"\").trim()) // Remove the \"data: \" prefix\n    .filter((line) => line !== \"\" && line !== \"[DONE]\") // Remove empty lines and \"[DONE]\"\n    .map((line) => {\n        try {\n            return JSON.parse(line)\n        } catch { }\n    }) // Parse the JSON string\n    .filter(Boolean);\n\nfor (const parsedLine of parsedLines) {\n    const { choices } = parsedLine;\n    const { delta } = choices[0];\n    const { content } = delta;\n    // Update the UI with the new content\n    if (content) {\n        resultTexts.push(content);\n    }\n}\nreturn resultTexts.join(\"\");",
      "sanatization_response": "// catch error\nif (res.status >= 300) {\n  const err = data?.error?.message || JSON.stringify(data);\n  throw err;\n}\n\n// get choices\nconst choices = (data.choices || data).map(c=> c.message);\n\n// the return object should be in the format of \n// { content: string }[] \n// if there's only one response, put it in the array of choices.\nreturn choices;",
      "frequency_penalty": 0,
      "presence_penalty": 0.5,
      "top_p": 1,
      "api_key": ""
    },
    "Chat Anthropic (Langchain)": {
      "basePath": "https://api.openai-hk.com",
      "model": "claude-3-5-sonnet-20240620"
    },
    "MistralAI Chat (Langchain)": {
      "basePath": "https://api.mistral.ai/v1"
    },
    "Google Palm (Langchain)": {},
    "Anthropic Legacy (Custom)": {
      "endpoint": "https://api.anthropic.com/v1/messages",
      "custom_header": "{\n    \"anthropic-version\": \"2023-06-01\",\n    \"Content-Type\": \"application/json\",\n    \"x-api-key\":  \"{{api_key}}\"\n}",
      "custom_body": "{\n    model: \"{{model}}\",\n    stream: {{stream}},\n    max_tokens: {{max_tokens}},\n    messages: {{stringify messages}}\n}",
      "model": "claude-2.1",
      "n": 1,
      "sanatization_streaming": "// catch error\nif (res.status >= 300) {\n  const err = data?.error?.message || JSON.stringify(data);\n  throw err;\n}\nlet resultTexts = [];\nconst lines = this.chunk.split(\"\\ndata: \");\n\nconst parsedLines = lines\n    .map((line) => line.replace(/^data: /, \"\").trim()) // Remove the \"data: \" prefix\n    .filter((line) => line !== \"\" && line !== \"[DONE]\") // Remove empty lines and \"[DONE]\"\n    .map((line) => {\n        try {\n            return JSON.parse(line)\n        } catch { }\n    }) // Parse the JSON string\n    .filter(Boolean);\n\nfor (const parsedLine of parsedLines) {\n    const { choices } = parsedLine;\n    const { delta } = choices[0];\n    const { content } = delta;\n    // Update the UI with the new content\n    if (content) {\n        resultTexts.push(content);\n    }\n}\nreturn resultTexts.join(\"\");",
      "sanatization_response": "async (data, res)=>{\n    // catch error\n    if (res.status >= 300) {\n      const err = data?.error?.message || JSON.stringify(data);\n      throw err;\n    }\n  \n    // get choices\n    const choices = data.content.map(c=> ({role:\"assistant\", content:c.text}));\n  \n    // the return object should be in the format of \n    // { content: string }[] \n    // if there's only one response, put it in the array of choices.\n    return choices;\n  }",
      "CORSBypass": true,
      "streamable": false
    },
    "Azure OpenAI Chat (Langchain)": {},
    "Azure OpenAI Instruct (Langchain)": {},
    "Google GenerativeAI (Langchain) 1": {
      "model": "gemini-1.5-flash",
      "api_key": ""
    }
  },
  "LLMProviderOptionsKeysHashed": {
    "Default (Custom).api_key": "__@#key_prefix#@__sk-THMCFLBo1EpHYuh53EGUUC3f6kxNK1iMNnXYWIiRgmzSskZy",
    "OpenAI Chat (Langchain).api_key": "__@#key_prefix#@__hk-4q599510000324641ab408e12c33f80ae3131740e6f68490",
    "Default (Custom) 1.api_key": "__@#key_prefix#@__sk-THMCFLBo1EpHYuh53EGUUC3f6kxNK1iMNnXYWIiRgmzSskZy",
    "Default (Custom) 1 2.api_key": "__@#key_prefix#@__sk-THMCFLBo1EpHYuh53EGUUC3f6kxNK1iMNnXYWIiRgmzSskZy",
    "Google GenerativeAI (Langchain).api_key": "__@#key_prefix#@__AIzaSyDQkRiqOvODmD8mXFOl0LoNFSZ9Qn6DR3M",
    "Google GenerativeAI (Langchain) 1.api_key": "__@#key_prefix#@__AIzaSyCfKbW26hd3EIX0ra7eFJSbM-RE8q8sICE"
  },
  "api_key_encrypted": "__@#key_prefix#@__hk-4q599510000324641ab408e12c33f80ae3131740e6f68490"
}